# -*- coding: utf-8 -*-
"""RNN_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a97yK4Ode5PyKAJ8HNhGtzNWLNUFw1xF
"""

special_character = 'X'
input_file_path = '/content/only_bengali.txt'
output_file_path = 'special.txt'

with open(input_file_path, 'r', encoding='utf-8') as input_file, open(output_file_path, 'w', encoding='utf-8') as output_file:
    output_file.writelines([special_character + line for line in input_file])

print("Special character added to the beginning of each sentence.")

!pip install nltk

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from nltk.tokenize import word_tokenize
from collections import Counter
import nltk
nltk.download('punkt')

with open('/content/special.txt', 'r', encoding='utf-8') as file:
    sentences = file.readlines()
tokenized_sentences = [word_tokenize(sentence) for sentence in sentences]

word_counts = Counter([word for sentence in tokenized_sentences for word in sentence])
vocab = {word: idx for idx, (word, _) in enumerate(word_counts.most_common())}
vocab['<UNK>'] = len(vocab)
vocab_size = len(vocab)
print(f"Vocabulary size: {vocab_size}")

max_seq_length = 50

padded_sequences = torch.zeros((len(tokenized_sentences), max_seq_length), dtype=torch.long)
for i, sentence in enumerate(tokenized_sentences):
    sequence = [vocab.get(word, vocab['<UNK>']) for word in sentence[:max_seq_length]]
    padded_sequences[i, :len(sequence)] = torch.tensor(sequence)

train_ratio = 0.8
train_size = int(train_ratio * len(sentences))
train_data = padded_sequences[:train_size]
test_data = padded_sequences[train_size:]

total_words_train = sum(len(sentence) for sentence in train_data)
total_words_test = sum(len(sentence) for sentence in test_data)

print(f"Total words in training dataset: {total_words_train}")
print(f"Total words in testing dataset: {total_words_test}")


train_file_path = "train_data.txt"
test_file_path = "test_data.txt"

with open(train_file_path, "w") as train_file:
    for sentence in train_data:
        sentence_text = " ".join(map(str, sentence))
        train_file.write(sentence_text + "\n")


with open(test_file_path, "w") as test_file:
    for sentence in test_data:
        sentence_text = " ".join(map(str, sentence))
        test_file.write(sentence_text + "\n")

batch_size = 64
input_dim = 128
hidden_dim = 256
num_layers = 2
num_epochs = 10
learning_rate = 0.001

print(f"Input data shape: ({max_seq_length}, {batch_size}, {input_dim})")

train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)
test_loader = DataLoader(TensorDataset(test_data), batch_size=batch_size, shuffle=False)

class RNNModel(nn.Module):
    def __init__(self, vocab_size, input_dim, hidden_dim, num_layers):
        super(RNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, input_dim)
        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output, _ = self.rnn(embedded)
        output = self.fc(output)
        return output

model = RNNModel(vocab_size, input_dim, hidden_dim, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
train_data = train_data.to(device)
test_data = test_data.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    for batch in train_loader:
        optimizer.zero_grad()
        inputs = batch[0].to(device)
        targets = inputs[:, 1:]
        outputs = model(inputs)
        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), targets.reshape(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()


    average_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch + 1}/{num_epochs}] - Average Loss: {average_loss:.4f}")

model.eval()

print("Training completed. You can print the evaluation results here.")

model_save_path = 'rnn_model.pth'
torch.save(model.state_dict(), model_save_path)
print(f"Word_level_Model saved to {model_save_path}")

import torch

num_epochs = 10

total_loss_10th_epoch = 0.0
total_tokens_10th_epoch = 0

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0

    for batch in test_loader:
        optimizer.zero_grad()
        inputs = batch[0].to(device)
        targets = inputs[:, 1:]
        outputs = model(inputs)
        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), targets.reshape(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        total_tokens = targets.numel()
        print(len(total_tokens))
        total_tokens_10th_epoch += total_tokens

    if epoch == 9:
        total_loss_10th_epoch = total_loss

average_loss_10th_epoch = total_loss_10th_epoch / len(train_loader)
print(f"Cross-Entropy Loss in the 10th epoch: {average_loss_10th_epoch:.4f}")

normalized_loss = total_loss_10th_epoch / total_tokens_10th_epoch
perplexity_10th_epoch = torch.exp(torch.tensor(normalized_loss))
print(f"Perplexity on the 10th epoch: {perplexity_10th_epoch:.4f}")

def remove_padding(line):

    parts = line.split('\t')
    trimmed_parts = [part.strip() for part in parts]

    cleaned_line = '\t'.join(trimmed_parts)

    return cleaned_line

input_file = '/content/test_data.txt'
output_file = 'paded_removed_word_test_data.txt'

with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:
    for line in infile:
        cleaned_line = remove_padding(line)

        outfile.write(cleaned_line + '\n')

print("Padding removed and saved to", output_file)

def calculate_perplexity(model, dataset, vocab_size, device):

    with open(dataset, 'r', encoding='utf-8') as file:
        sentences = file.readlines()

    normalized_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(normalized_loss))

    return perplexity

model_path = '/content/rnn_model.pth'
dataset_path = input("Enter the path to the dataset file: ")

perplexity = calculate_perplexity(model, dataset_path, vocab_size, device)

print(f"Perplexity on the provided dataset: {perplexity:.4f}")















import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from collections import Counter
import string

with open('/content/special.txt', 'r', encoding='utf-8') as file:
    text = file.read()

tokenized_text = list(text)

vocab = {char: idx for idx, char in enumerate(set(tokenized_text))}
vocab['<UNK>'] = len(vocab)

vocab_size = len(vocab)
print(f"Vocabulary size: {vocab_size}")

max_seq_length = 50

indexed_text = [vocab.get(char, vocab['<UNK>']) for char in tokenized_text]

num_batches = len(indexed_text) // max_seq_length
padded_sequences = torch.zeros((num_batches, max_seq_length), dtype=torch.long)
for i in range(num_batches):
    start_idx = i * max_seq_length
    end_idx = start_idx + max_seq_length
    sequence = indexed_text[start_idx:end_idx]
    padded_sequences[i, :len(sequence)] = torch.tensor(sequence)

data = padded_sequences

train_ratio = 0.8
train_size = int(train_ratio * len(data))

train_data = data[:train_size]
test_data = data[train_size:]

total_chars_train = len(train_data.view(-1))
total_chars_test = len(test_data.view(-1))

print(f"Total characters in training dataset: {total_chars_train}")
print(f"Total characters in testing dataset: {total_chars_test}")

train_file_path = "train_char_data.txt"
test_file_path = "test_char_data.txt"

with open(train_file_path, "w") as train_file:
    for sentence in train_data:
        sentence_text = " ".join(map(str, sentence))
        train_file.write(sentence_text + "\n")


with open(test_file_path, "w") as test_file:
    for sentence in test_data:
        sentence_text = " ".join(map(str, sentence))
        test_file.write(sentence_text + "\n")

batch_size = 64
input_dim = 128
hidden_dim = 256
num_layers = 2
num_epochs = 10
learning_rate = 0.001
print(f"Input data shape: ({batch_size}, {max_seq_length})")

train_loader = DataLoader(TensorDataset(train_data), batch_size=batch_size, shuffle=True)
test_loader = DataLoader(TensorDataset(test_data), batch_size=batch_size, shuffle=False)

class CharRNNModel(nn.Module):
    def __init__(self, vocab_size, input_dim, hidden_dim, num_layers):
        super(CharRNNModel, self).__init__()
        self.embedding = nn.Embedding(vocab_size, input_dim)
        self.rnn = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)
        self.fc = nn.Linear(hidden_dim, vocab_size)

    def forward(self, x):
        embedded = self.embedding(x)
        output, _ = self.rnn(embedded)
        output = self.fc(output)
        return output

model = CharRNNModel(vocab_size, input_dim, hidden_dim, num_layers)
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
train_data = train_data.to(device)
test_data = test_data.to(device)

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0
    for batch in train_loader:
        optimizer.zero_grad()
        inputs = batch[0].to(device)
        targets = inputs[:, 1:]
        outputs = model(inputs)
        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), targets.reshape(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    average_loss = total_loss / len(train_loader)
    print(f"Epoch [{epoch + 1}/{num_epochs}] - Average Loss: {average_loss:.4f}")

model.eval()

print("Training completed..")

model_save_path = 'char_rnn_model.pth'
torch.save(model.state_dict(), model_save_path)
print(f"Char_Model saved to {model_save_path}")

import torch

num_epochs = 10

total_loss_10th_epoch = 0.0
total_tokens_10th_epoch = 0

for epoch in range(num_epochs):
    model.train()
    total_loss = 0.0

    for batch in test_loader:
        optimizer.zero_grad()
        inputs = batch[0].to(device)
        targets = inputs[:, 1:]
        outputs = model(inputs)
        loss = criterion(outputs[:, :-1].reshape(-1, vocab_size), targets.reshape(-1))
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
        total_tokens = targets.numel()
        total_tokens_10th_epoch += total_tokens

    if epoch == 9:
        total_loss_10th_epoch = total_loss

average_loss_10th_epoch = total_loss_10th_epoch / len(train_loader)
print(f"Cross-Entropy Loss in the 10th epoch: {average_loss_10th_epoch:.4f}")

normalized_loss = total_loss_10th_epoch / total_tokens_10th_epoch
perplexity_10th_epoch = torch.exp(torch.tensor(normalized_loss))
print(f"Perplexity on the 10th epoch: {perplexity_10th_epoch:.4f}")

def remove_padding(line):

    parts = line.split('\t')
    trimmed_parts = [part.strip() for part in parts]

    cleaned_line = '\t'.join(trimmed_parts)

    return cleaned_line

input_file = '/content/test_char_data.txt'
output_file = 'paded_removed_char_test_data.txt'

with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:
    for line in infile:
        cleaned_line = remove_padding(line)

        outfile.write(cleaned_line + '\n')

print("Padding removed and saved to", output_file)

def calculate_perplexity(model, dataset, vocab_size, device):

    with open(dataset, 'r', encoding='utf-8') as file:
        sentences = file.readlines()

    normalized_loss = total_loss / total_tokens
    perplexity = torch.exp(torch.tensor(normalized_loss))

    return perplexity

model_path = '/content/char_rnn_model.pth'
dataset_path = input("Enter the path to the dataset file: ")

perplexity = calculate_perplexity(model, dataset_path, vocab_size, device)

print(f"Perplexity on the provided dataset: {perplexity:.4f}")
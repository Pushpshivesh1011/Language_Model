{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM697iUUHH0X2DLMxJ9Xsus",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pushpshivesh1011/Language_Model/blob/main/Cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install spacy\n"
      ],
      "metadata": {
        "id": "_B9PEVgQfJyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import emoji\n",
        "import indicnlp\n",
        "from spacy.lang.hi import Hindi\n"
      ],
      "metadata": {
        "id": "fGyJP1UAbgU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_hindi_text(text):\n",
        "\n",
        "  text = text.strip()\n",
        "\n",
        "  text = re.sub(r'(?:\\s+|[â€”''\",_])+', ' ', text)\n",
        "\n",
        "  text = re.sub(r'\\n+', '\\n', text)\n",
        "\n",
        "  text = re.sub(r'^\\s+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "  text = re.sub(r'^[^\\w\\s]', '', text, flags=re.MULTILINE)\n",
        "\n",
        "  seen = set()\n",
        "  cleaned_sentences = []\n",
        "  for sentence in text.splitlines():\n",
        "    if sentence not in seen:\n",
        "      cleaned_sentences.append(sentence)\n",
        "      seen.add(sentence)\n",
        "\n",
        "  text = re.sub(r'\\$\\$.*?\\$\\$', '', text)\n",
        "\n",
        "  text = re.sub(r'\\d+[\\+\\-\\*/]\\d+', '', text)\n",
        "\n",
        "  text = emoji.replace_emoji(text)\n",
        "\n",
        "  text = re.sub(r'\\W+\\b[^\\u0900-\\u097F\\s]+\\b', '', text)\n",
        "\n",
        "  text = re.sub(r'\\.+', '.', text)\n",
        "\n",
        "  text = re.sub(r'^\\d+\\.\\s+', '', text, flags=re.MULTILINE)\n",
        "\n",
        "  text = re.sub(r'\\n+', '\\n', text)\n",
        "\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "KrdPU1LlbgYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_to_long_sentence(sentence):\n",
        "\n",
        "  words = sentence.split()\n",
        "  new_sentence = []\n",
        "  for word in words:\n",
        "    new_sentence.append(word)\n",
        "    if len(new_sentence) > 25:\n",
        "      return ' '.join(new_sentence)\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "a9MBnEp8BrOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_paragraphs_to_sentences(text):\n",
        "  nlp = Hindi()\n",
        "  doc = nlp(text)\n",
        "  sentences = []\n",
        "  for sent in doc.sents:\n",
        "    sentences.append(sent.string.strip())\n",
        "  return sentences"
      ],
      "metadata": {
        "id": "PuPc5bGk2_9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/hindi_extra_all.txt', 'r') as f:\n",
        "  hindi_sentences = f.readlines()\n"
      ],
      "metadata": {
        "id": "9uH1fCcMbga1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_hindi_sentences = []\n",
        "for sentence in hindi_sentences:\n",
        "  cleaned_hindi_sentences.append(clean_hindi_text(sentence))\n"
      ],
      "metadata": {
        "id": "lqrxgjoobuRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('hindi_sentences_cleaned.txt', 'w') as f:\n",
        "  for sentence in cleaned_hindi_sentences:\n",
        "    f.write(sentence + '\\n')\n"
      ],
      "metadata": {
        "id": "hNKauPmTbztV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "IV3VuipVnyrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def tokenize_long_sentences(sentences):\n",
        "\n",
        "  long_sentences = [sentence for sentence in sentences if len(sentence.split()) > 30]\n",
        "\n",
        "  from nltk.tokenize import sent_tokenize\n",
        "\n",
        "  tokenized_long_sentences = []\n",
        "  for sentence in long_sentences:\n",
        "    tokenized_long_sentences.extend(sent_tokenize(sentence))\n",
        "\n",
        "  return tokenized_long_sentences\n",
        "\n",
        "def clean_hindi_sentences(dataset_path, output_path):\n",
        "\n",
        "  with open(dataset_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = f.readlines()\n",
        "\n",
        "  cleaned_dataset = []\n",
        "  for sentence in dataset:\n",
        "\n",
        "    sentence = re.sub(r\"([.!?])(?=\\s+)\", r\"\\1\\n\", sentence)\n",
        "\n",
        "    if re.match(r\"^[^\\w\\s]+$\", sentence):\n",
        "      continue\n",
        "\n",
        "    sentence = re.sub(r\"^[^\\w\\s]\", \"\", sentence)\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    sentence = re.sub(r\"\\d\", \"\", sentence)\n",
        "\n",
        "    sentence = re.sub(r\"([,\\n\\.!?()])+\", r\"\\1\", sentence)\n",
        "\n",
        "    cleaned_dataset.append(sentence)\n",
        "\n",
        "  with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    for sentence in cleaned_dataset:\n",
        "      f.write(sentence + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  dataset_path = input(\"Enter the path to the input dataset in .txt format: \")\n",
        "  output_path = input(\"Enter the path to the output dataset in .txt format: \")\n",
        "\n",
        "  clean_hindi_sentences(dataset_path, output_path)\n",
        "\n",
        "  print(\"The dataset has been cleaned and saved to the output file.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPaHIQHCH-7Z",
        "outputId": "416acb31-75be-44f1-c662-1ff6af797e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the path to the input dataset in .txt format: /content/hindi_sentences_cleaned.txt\n",
            "Enter the path to the output dataset in .txt format: /content/hindi_extra_all.txt\n",
            "The dataset has been cleaned and saved to the output file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head -5000 '/content/cleaned_dataset.txt'"
      ],
      "metadata": {
        "id": "rzNS59abG4j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def counter(fname):\n",
        "\n",
        "\tnum_words = 0\n",
        "\tnum_lines = 0\n",
        "\tnum_charc = 0\n",
        "\tnum_spaces = 0\n",
        "\n",
        "\twith open(fname, 'r') as f:\n",
        "\t\tfor line in f:\n",
        "\n",
        "\t\t\tnum_lines += 1\n",
        "\t\t\tword = 'Y'\n",
        "\n",
        "\t\t\tfor letter in line:\n",
        "\t\t\t\tif (letter != ' ' and word == 'Y'):\n",
        "\t\t\t\t\tnum_words += 1\n",
        "\t\t\t\t\tword = 'N'\n",
        "\t\t\t\telif (letter == ' '):\n",
        "\t\t\t\t\tnum_spaces += 1\n",
        "\t\t\t\t\tword = 'Y'\n",
        "\n",
        "\t\t\t\tfor i in letter:\n",
        "\t\t\t\t\tif(i !=\" \" and i !=\"\\n\"):\n",
        "\t\t\t\t\t\tnum_charc += 1\n",
        "\n",
        "\tprint(\"Number of words in text file: \",\n",
        "\t\tnum_words)\n",
        "\tprint(\"Number of lines in text file: \",\n",
        "\t\tnum_lines)\n",
        "\tprint('Number of characters in text file: ',\n",
        "\t\tnum_charc)\n",
        "\tprint('Number of spaces in text file: ',\n",
        "\t\tnum_spaces)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tfname = '/content/hindi_extra_all.txt'\n",
        "\ttry:\n",
        "\t\tcounter(fname)\n",
        "\texcept:\n",
        "\t\tprint('File not found')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACSVc2vYCM1T",
        "outputId": "1bc1e89a-0a0f-4b1a-ba7d-4ab282f70826"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in text file:  6070565\n",
            "Number of lines in text file:  238246\n",
            "Number of characters in text file:  25453420\n",
            "Number of spaces in text file:  5846585\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def counter(fname):\n",
        "\n",
        "\tnum_words = 0\n",
        "\tnum_lines = 0\n",
        "\tnum_charc = 0\n",
        "\tnum_spaces = 0\n",
        "\n",
        "\twith open(fname, 'r') as f:\n",
        "\t\tfor line in f:\n",
        "\n",
        "\t\t\tnum_lines += 1\n",
        "\t\t\tword = 'Y'\n",
        "\n",
        "\t\t\tfor letter in line:\n",
        "\t\t\t\tif (letter != ' ' and word == 'Y'):\n",
        "\t\t\t\t\tnum_words += 1\n",
        "\t\t\t\t\tword = 'N'\n",
        "\t\t\t\telif (letter == ' '):\n",
        "\t\t\t\t\tnum_spaces += 1\n",
        "\t\t\t\t\tword = 'Y'\n",
        "\n",
        "\t\t\t\tfor i in letter:\n",
        "\t\t\t\t\tif(i !=\" \" and i !=\"\\n\"):\n",
        "\t\t\t\t\t\tnum_charc += 1\n",
        "\n",
        "\tprint(\"Number of words in text file: \",\n",
        "\t\tnum_words)\n",
        "\tprint(\"Number of lines in text file: \",\n",
        "\t\tnum_lines)\n",
        "\tprint('Number of characters in text file: ',\n",
        "\t\tnum_charc)\n",
        "\tprint('Number of spaces in text file: ',\n",
        "\t\tnum_spaces)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\tfname = '/content/cleaned_dataset.txt'\n",
        "\ttry:\n",
        "\t\tcounter(fname)\n",
        "\texcept:\n",
        "\t\tprint('File not found')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJZK500RFWZK",
        "outputId": "34f8bb9f-cede-4e84-c56c-c57033a9d4e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of words in text file:  5959165\n",
            "Number of lines in text file:  269277\n",
            "Number of characters in text file:  24720732\n",
            "Number of spaces in text file:  5727867\n"
          ]
        }
      ]
    }
  ]
}